{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c1b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt configuration\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"n_layers\": 12,   # number of transformer blocks\n",
    "    \"n_heads\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"context_length\": 1024,\n",
    "    \"embed_dim\": 768,\n",
    "    \"vocab_size\": 50257,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8c0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    '''normalizing is adjusting the mean and std of the input'''\n",
    "\n",
    "    # layer norm is applied on feature dimension\n",
    "    # vs batch norm depends on the batch dimension\n",
    "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        # 1+eps to avoid division by zero\n",
    "        return (x - mean) / torch.sqrt(var + 1e-5)\n",
    "\n",
    "\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized = self.normalize(x)\n",
    "        # just turnable knobs to adjust normalization during training\n",
    "        return normalized * self.scale + self.shift\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            # expansion\n",
    "            nn.Linear(config[\"embed_dim\"], 4 * config[\"embed_dim\"]),\n",
    "            # activation\n",
    "            GELU(),\n",
    "            # contraction\n",
    "            nn.Linear(4 * config[\"embed_dim\"], config[\"embed_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class MultiheadAttention1(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # check if d_in is divisible by num_heads\n",
    "        assert d_in % num_heads == 0, \"d_in must be divisible by num_heads\"\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # x dims = batch_size, num_tokens, d_in\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        q = self.W_query(x)         # batch_size, num_tokens, d_out\n",
    "        k = self.W_key(x)           # batch_size, num_tokens, d_out\n",
    "        v = self.W_value(x)         # batch_size, num_tokens, d_out\n",
    "\n",
    "        # split the query, key and value into multiple heads\n",
    "        q = q.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        k = k.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        v = v.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # transpose the query to batch_size, num_heads, num_tokens, head_dim\n",
    "\n",
    "        #  new dims = batch_size, num_heads, num_tokens, head_dim\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "     \n",
    "        # calculate the attention scores = num_tokens x num_tokens\n",
    "        attention_scores = q @ k.transpose(2, 3)\n",
    "\n",
    "        # apply the mask to the attention scores\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # apply the softmax to the attention scores\n",
    "        attention_weights = torch.softmax(attention_scores / self.head_dim**0.5, dim=-1)    \n",
    "\n",
    "        # drop out the attention weights\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # calculate the context vector\n",
    "        # attention_weights dims = batch_size, num_heads, num_tokens, num_tokens\n",
    "        # v dims = batch_size, num_heads, num_tokens, head_dim\n",
    "\n",
    "        # new dims = batch_size, num_heads, num_tokens, head_dim\n",
    "        context_vector = attention_weights @ v\n",
    "\n",
    "        # new dims = batch_size, num_tokens, num_heads, head_dim\n",
    "        context_vector = context_vector.transpose(1, 2)\n",
    "\n",
    "        # merge the heads\n",
    "        # new dims = batch_size, num_tokens, d_out\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        # project the context vector to the output dimension\n",
    "        # new dims = batch_size, num_tokens, d_out\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.att = MultiheadAttention1(\n",
    "            d_in=config[\"embed_dim\"],\n",
    "            d_out=config[\"embed_dim\"],\n",
    "            num_heads=config[\"n_heads\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            dropout=config[\"drop_rate\"],\n",
    "            qkv_bias=config[\"qkv_bias\"],\n",
    "        )\n",
    "        \n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "        self.norm1 = LayerNorm(config[\"embed_dim\"])\n",
    "        self.norm2 = LayerNorm(config[\"embed_dim\"])\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # token embedding matrix\n",
    "        self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"embed_dim\"])\n",
    "\n",
    "        # positional embedding\n",
    "        self.positional_embedding = nn.Embedding(config[\"context_length\"], config[\"embed_dim\"])\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "\n",
    "        # transformer blocks\n",
    "        self.transformer_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(config) for _ in range(config[\"n_layers\"])\n",
    "        ])\n",
    "\n",
    "        # final layer norm\n",
    "        self.final_layer_norm = LayerNorm(config[\"embed_dim\"])\n",
    "\n",
    "        self.output_head = nn.Linear(config[\"embed_dim\"], config[\"vocab_size\"], bias=False)\n",
    "        \n",
    "\n",
    "    def forward(self, input_idx):\n",
    "        input_idx_batch_sz, input_idx_len = input_idx.shape\n",
    "\n",
    "        # create the embedding matrix\n",
    "        token_embeddings = self.token_embedding(input_idx)\n",
    "        positional_embeddings = self.positional_embedding(torch.arange(input_idx_len))\n",
    "\n",
    "        vector_embeddings = token_embeddings + positional_embeddings\n",
    "\n",
    "\n",
    "        # drop out\n",
    "        vector_embeddings = self.dropout(vector_embeddings)\n",
    "\n",
    "        # transformer blocks\n",
    "        transformer_output = self.transformer_blocks(vector_embeddings)\n",
    "\n",
    "        # final layer norm\n",
    "        normalized_output = self.final_layer_norm(transformer_output)\n",
    "\n",
    "        # output head\n",
    "        logits = self.output_head(normalized_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c7a5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of output:  torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# test trasnformer with dummy data batch size 2\n",
    "\n",
    "tb = torch.randn(2, 4, 768)\n",
    "\n",
    "transformer_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "output = transformer_block(tb)\n",
    "print(\"shape of output: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc84fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in ./.conda/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.conda/lib/python3.12/site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.conda/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b626490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6109, 3626, 6100, 345]\n",
      "[6109, 1110, 6622, 257]\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "shape of tb:  torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "sentence1 = \"Every effort moves you\"\n",
    "sentence2 = \"Every day holds a\"\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens1 = enc.encode(sentence1)\n",
    "print(tokens1)\n",
    "\n",
    "tokens2 = enc.encode(sentence2)\n",
    "print(tokens2)\n",
    "\n",
    "\n",
    "batch = [torch.tensor(tokens1), torch.tensor(tokens2)]\n",
    "tb = torch.stack(batch, dim=0)\n",
    "print(tb)\n",
    "print(\"shape of tb: \", tb.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61c6897",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DummyGPTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dummy_gpt = \u001b[43mDummyGPTModel\u001b[49m(GPT_CONFIG_124M)\n\u001b[32m      3\u001b[39m output = dummy_gpt(tb)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mshape of output: \u001b[39m\u001b[33m\"\u001b[39m, output.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'DummyGPTModel' is not defined"
     ]
    }
   ],
   "source": [
    "dummy_gpt = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "output = dummy_gpt(tb)\n",
    "print(\"shape of output: \", output.shape)\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c32c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0610], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# shortcut connection tutorial \n",
    "\n",
    "# it will make gradient smoother, and help with vanishing gradient problem\n",
    "\n",
    "class ShortcutConnection(nn.Module):\n",
    "    def __init__(self, layer_sizes: list[int], use_shortcut: bool):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.use_shortcut = use_shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "\n",
    "        return x\n",
    "\n",
    "layer_size = [3, 3, 3, 3, 3, 1]\n",
    "use_shortcut = False\n",
    "\n",
    "torch.manual_seed(123)\n",
    "shortcut_connection = ShortcutConnection(layer_size, use_shortcut)\n",
    "\n",
    "x = torch.tensor([1.0, 0.0, -1.0])\n",
    "\n",
    "print(shortcut_connection(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dfd38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight 0.0006052076350897551\n",
      "layers.1.0.weight 0.000360334845026955\n",
      "layers.2.0.weight 0.0021456119138747454\n",
      "layers.3.0.weight 0.004196620546281338\n",
      "layers.4.0.weight 0.01514893677085638\n"
     ]
    }
   ],
   "source": [
    "def print_gradient(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([0.0])\n",
    "    loss = nn.MSELoss()(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # print mean absolute value of gradient\n",
    "            print(name, param.grad.abs().mean().item())\n",
    "\n",
    "\n",
    "print_gradient(shortcut_connection, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03462716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight 0.2223031222820282\n",
      "layers.1.0.weight 0.20684535801410675\n",
      "layers.2.0.weight 0.3305492103099823\n",
      "layers.3.0.weight 0.2698953449726105\n",
      "layers.4.0.weight 1.3107051849365234\n"
     ]
    }
   ],
   "source": [
    "# use_shortcut = True\n",
    "shortcut_connection.use_shortcut = True\n",
    "print_gradient(shortcut_connection, x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2a1805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([2, 4])\n",
      "output:  tensor([[[ 0.3613,  0.4223, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "output.shape:  torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "# run gpt \n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "gpt_model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "output = gpt_model(tb)\n",
    "\n",
    "print(\"input shape: \", tb.shape)\n",
    "\n",
    "print(\"output: \", output)\n",
    "\n",
    "print(\"output.shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6020adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters:  163009536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in gpt_model.parameters())\n",
    "print(\"total parameters: \", total_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62903c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate sample tokens\n",
    "\n",
    "def generate_tokens(model, idx, max_input_tokens, context_size):\n",
    "\n",
    "#     # idx shape is batch_size, num_tokens\n",
    "#     # tensor([[6109, 3626, 6100,  345],\n",
    "#         [6109, 1110, 6622,  257]])\n",
    "    for _ in range(max_input_tokens):\n",
    "\n",
    "        # look at the last context_size tokens\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # batch, number of tokens, vocab_size\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) \n",
    "\n",
    "\n",
    "        # get last rows from each batch because that is our prediction\n",
    "        last_rows = logits[:, -1, :]\n",
    "\n",
    "        # get the probability which is most likely to be the next token\n",
    "        probs = torch.softmax(last_rows, dim=-1)\n",
    "\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        # append the next token to the context\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf13ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,    11,   314,   716, 27018, 34305, 14838,  2694, 20485, 18702]])\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello, I am\"\n",
    "tokens = enc.encode(sample_text)\n",
    "encoded_tokens = torch.tensor(tokens).unsqueeze(0)\n",
    "\n",
    "gpt_model.eval()\n",
    "\n",
    "token_output = generate_tokens(model=gpt_model, idx=encoded_tokens, max_input_tokens=6, context_size=4)\n",
    "\n",
    "print(token_output)\n",
    "\n",
    "# decode the tokens\n",
    "decoded_tokens = enc.decode(token_output[0].tolist())\n",
    "print(decoded_tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fac014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
